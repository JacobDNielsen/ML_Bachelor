{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science and Visualization (RUC F2023)\n",
    "\n",
    "## Miniproject Report: NLP for review evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Members:\n",
    "* William Grynderup Klindt, RUC, 2023, wgk@ruc.dk\n",
    "* Luchas Schmidt, RUC, 2023, luchas@ruc.dk\n",
    "* Jacob Peter Diesel Nielsen, RUC, 2023, jpn@ruc.dk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "This project aims to classify the sentiment of reviews using an existing dataset of product reviews from Amazon. We used the dataset to train two supervised models, namely Support Vector Machine and Naive Bayes. Through various data visualization techniques, we investigated and analyzed which model performed better with our dataset and discussed the pros and cons of our results and approaches. We conclude that the Support vector Machine algorithm is generally better due to its flexibility and that we are content with our models accuracy of approximately 70%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Data\n",
    "\n",
    "Import the modules needed for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/home/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/home/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk ## If the project can't run, you might have to install NLTK. Please follow the instructions here: https://www.nltk.org/install.html \n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stopword=set(stopwords.words('english')) \n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data description:\n",
    "This dataset contains approximately 568.000 online Amazon reviews for different amazon products. We have minimized the dataset to approximately 10.000 reviews because we seek to prevent computational and runtime issues.\n",
    "Here is the link for the dataset: https://www.kaggle.com/datasets/jillanisofttech/amazon-product-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Data\n",
    "\n",
    "Our dataset consists of 10.114 rows and 10 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/luchasschmidt/Git/ML_Bachelor/Reviews_small.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/91/y_nk13s14zvf_g080jglym3c0000gn/T/ipykernel_12851/179762864.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/luchasschmidt/Git/ML_Bachelor/Reviews_small.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/luchasschmidt/Git/ML_Bachelor/Reviews_small.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/luchasschmidt/Git/ML_Bachelor/Reviews_small.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a sample of the full dataset which consists of rows that display each review in the dataset and each column displays the following:\n",
    "- ID\n",
    "- The product ID\n",
    "- The ID of the author who wrote the review\n",
    "- The author's username\n",
    "- Numerator for helpfulness of review\n",
    "- Denominator for helpfulness of review\n",
    "- Product rating\n",
    "- Review time\n",
    "- A title of the review\n",
    "- A descriptive text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "Since we downloaded the dataset from Kaggle, they ensured that we received a dataset with zero missing values. \n",
    "Next, we have decided to drop some of the columns that we found irrelevant for the project.\n",
    "\n",
    "These are the columns we removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = data.drop(labels=['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time', 'Summary'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As displayed below, the only columns that we found relevant was the \"Score\" and \"Text\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify reviews as positive, neutral, or negative, we have created a function that evaluates the score of each review. If the review has a score of 3, we consider it to be neutral. If the score is below 3, such as 1 or 2, we consider it to be a negative review. If the score is greater than 3, we consider it to be a positive review.\n",
    "\n",
    "Below the function, we use a lambda expression to apply the function to each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentiment(rating):\n",
    "    if rating == 3:\n",
    "        return 1\n",
    "    elif rating in [1, 2]:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "data['label'] = data['Score'].apply(lambda x: classify_sentiment(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then print the name of each column in our dataset to verify that the functions worked as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset now consists of reviews with labels to define the positive, negative, or neutral sentiment of each review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that each row contains a value for each column, we use the info() method from Pandas. We also verify that the \"label\" column only consists of integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning\n",
    "We seek to clean the text in our dataset to optimize the training model. We found that our text data contains various noise and unwanted characters, such as punctuation, stop words, and special characters, which could potentially affect the quality of the classification process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created two functions that are used for lemmatization of each word, meaning that each word should be changed to its root form. The first function, pos_tagger, defines which word class (POS) a given word belongs to. The function takes an NLTK tag as a parameter and returns a Wordnet POS tag. For example, if the input tag is a Noun, it will return a Wordnet Noun tag. If the input tag doesn't match any of the criteria, the function returns None.\n",
    "\n",
    "Next, we define the function lemmatize_sentence, which is used for lemmatizing sentences. First, we tokenize the sentence with the word_tokenize function from the NLTK library, and secondly, we assign a POS tag to each word with the pos_tag function from the NLTK library. Each POS tag is then converted to a Wordnet POS tag with the beforementioned pos_tagger function. The tokenized words (with their accordingly POS tags) are then stored in a list of tuples.\n",
    "\n",
    "Lastly, we iterate over the beforementioned list and perform lemmatization of each word with the use of the WordNetLemmatizer and its associated POS tag. If the tag is None (meaning it couldn't assign a tag), the word will be added without lemmatization. If the word got a tag, the word is lemmatized. For both cases, we also make sure that the word is not a stopword. If it is the case, then the word will not be added.\n",
    "\n",
    "[Reference: Lemmatizer](https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the POS tagger function with modifications for handling unavailable tags\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None  # Use noun as default tag if tag is unavailable''\n",
    "\n",
    "# Define the function to lemmatize each word in a sentence with its POS tag\n",
    "def lemmatize_sentence(sentence):\n",
    "    # Tokenize the sentence and find the POS tag for each token\n",
    "    pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "\n",
    "    # Use our own pos_tagger function to make things simpler to understand\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "\n",
    "    # Lemmatize each word with its POS tag, ignoring stopwords\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None and word.lower() not in stopword:\n",
    "            # If there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:\n",
    "            # Use the tag to lemmatize the token, ignoring stopwords\n",
    "            if word.lower() not in stopword:\n",
    "                lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "\n",
    "    # Join the lemmatized words into a sentence\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removes any characters that isn't included in the english alphabet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Text\"] = data['Text'].str.replace('[^a-zA-Z]',' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new column called \"lemma_text\" that removes all stop words and lemmatizes each word, meaning that each word should be changed to its root form. For instance, the word \"running\" should be changed to \"run\". This is done by calling lemmatize_sencence to the column \"Text\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemma_text'] = data.Text.apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we showcase the dataset before and after lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "In this chapter we aim to present informative visualizations of our dataset that highlight relevant aspects of the data. We will then analyze and interpret these visualizations to gain further insights into the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram showcasing the distribution of the original \"Score\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data.Score, align='mid', bins=[0.5, 1.5, 2.5, 3.5, 4.5, 5.5])\n",
    "plt.xticks([1, 2, 3, 4, 5])#, ['Negative', 'Neutral', 'Positive', 'Positive', 'Positive'])\n",
    "#plt.xlim([0.5, 5.5])\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piechart showcasing the distribution of the original \"Score\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "data['Score'].value_counts().plot.pie(autopct='%1.1f%%', startangle=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram showcasing the distribtuion of the \"label\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data.label, align='mid', bins=[-0.5, 0.5, 1.5, 2.5])\n",
    "plt.xticks([0, 1, 2], ['Negative', 'Neutral', 'Positive'])\n",
    "#plt.xlim([-0.5, 2.5])\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piechart showcasing the distribution of the \"label\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "data['label'].value_counts().plot.pie(autopct='%1.1f%%', startangle=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As displayed above, the majority of the labels are valued as positive, and thereby very unbalanced. The different visualizations highlights that our model might be prone to overfitting when trained on this dataset.  \n",
    "\n",
    "The majority of the positive class can bias the model towards certain patterns or phrases, potentially leading to poor generalization performance on the other two classes. The model may be likely to predict the majority class, even when the input text is from a different class.\n",
    "\n",
    "To ensure that we train our model properly, without any potential chances of overfitting or general poor performance, we want to balance out our dataset. \n",
    "\n",
    "We decided to fix this issue by splitting the score equally among the 5 ratings. Because of computational limits and runtime issues, we have chosen to use 2000 reviews from each rating. This made us use the data from the original dataset containing 568.000 ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/luchasschmidt/Git/ML_Bachelor/Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount = 2000\n",
    "\n",
    "grouped = data.groupby('Score')\n",
    "\n",
    "data = pd.DataFrame(columns=['Text', 'Score'])\n",
    "\n",
    "for label, group in grouped:\n",
    "    selected_group = group.head(amount)\n",
    "    data = pd.concat([data, selected_group])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we applied all of the previous mentioned methods for text cleaning on the large dataset. We will demonstrate how we accomplished this, but due to the dataset's size, the runtime was quite lengthy. Therefore, we only performed the process once and saved the result as a new dataset, which we named \"updated_dataframe.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(labels=['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time', 'Summary'], axis=1)\n",
    "data['label'] = data['Score'].apply(lambda x: classify_sentiment(x))\n",
    "data[\"Text\"] = data['Text'].str.replace('[^a-zA-Z]',' ', regex=True)\n",
    "data['lemma_text'] = data.Text.apply(lemmatize_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('updated_dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/luchasschmidt/Git/ML_Bachelor/updated_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the mentioned issue regarding overfitting we want to balance our dataset. We did that by updating the dataframe so it contains 2000 reviews from each class. The histogram below visualize the new and balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data.Score, align='mid', bins=[0.5, 1.5, 2.5, 3.5, 4.5, 5.5], width=0.8)\n",
    "plt.xticks([1, 2, 3, 4, 5])#, ['Negative', 'Neutral', 'Positive', 'Positive', 'Positive'])\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Modelling\n",
    "\n",
    "In this section of our report we aim to present our approach to data modelling. We first present how to select features for training and validate our model. Lastly, we discuss our findings and evaluate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We select the features we use in our model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data[\"lemma_text\"]) ## input data, object (string)\n",
    "y = np.array(data[\"label\"]) ## label, int64 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instatiating the two machine learning algorithms Naive Bayes and Support Vector Machines (SVM) for classification tasks.\n",
    "We are currently using both of these machine learning algorithms in our research for our Bachelor project, which lead us to also use the algorithms in this project. We are using Naive Bayes because of its simplicity, fast training time, and computational efficiency. The SVM algorithm is more powerful and gives us the ability to tweak our model in a more customiziable manner. However, we are using the same methods for training both algorithms and SVM should in theory be slower, but more accurate.\n",
    "\n",
    "[Reference: Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)\n",
    "<br>\n",
    "[Reference: Support Vector Machine](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB() # Instantierer Naives Bayes\n",
    "svm = SVC(kernel='linear') # Instantierer SVM, og sætter dens kernal til linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we split the data into training and test sets. We decide to use 20% of the data for testing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make use of count vectorizor since we are working with datasets which concists of words, we transform the words into numeric values. We have chosen an ngram range of 1,2, which means we represent word in the sentence as a singe or combination of two words. For instance the sentence \"I didn't like this shampoo\" is first changed by the ngram range to \"I, didn't, like, this, shampoo\". Secondly, the sentence is also stored as \"I didn't, I like, I this, I shampoo, didn't like, didn't this...\" etc. This continious for every combination of every two words.\n",
    "\n",
    "[Reference: Count Vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectCount = CountVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing of the before mentioned algorithms. Initially we create a list called models, storing the names of the algorithms we want to modelize.\n",
    "We then create a function called data_modelling, with two parameters. The first is models, which is the list of algorithms that we want to modelize. The second is vect, and specifics the method that should be used to represent words as numerical values. In our case we only use CountVectorizer as an argument, but other methods for representing the words could be used.\n",
    "\n",
    "Inside the function we iterate over a for loop according to the length of the models list. For each iteration, meaning for each algorithm, we transform the text to numerical values for both the training and test. We then train the model with the text represented as numerical values and their according label.\n",
    "After the first model is trained, the accuracy score gets appended to a list, and the next iteration starts. We finally append all the accuracy scores for both models, and return the list as a DataFrame. This gives a DataFrame showcasing the accuracy for the respective models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models=[nb, svm]\n",
    "\n",
    "def data_modelling(models,vect):\n",
    "    result_table=[]\n",
    "    for i in range(len(models)):\n",
    "        X_train_vect = vect.fit_transform(X_train)\n",
    "        X_test_vect = vect.transform(X_test)\n",
    "        model = models[i]\n",
    "        model.fit(X_train_vect, y_train)\n",
    "        y_pred_class = model.predict(X_test_vect)\n",
    "        \n",
    "        data={'Accuracy score':metrics.accuracy_score(y_test, y_pred_class)}\n",
    "        \n",
    "        result_table.append(data)  \n",
    "        \n",
    "    df = pd.DataFrame(result_table, index =['Naive Bayes', 'Support Vector Machine'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modelling(models, vectCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount = 200\n",
    "\n",
    "small_df = pd.DataFrame(columns=['Text', 'Score'])\n",
    "\n",
    "for label, group in grouped:\n",
    "    selected_group = group.head(amount)\n",
    "    small_df = pd.concat([small_df, selected_group])\n",
    "    \n",
    "small_df['label'] = small_df['Score'].apply(lambda x: classify_sentiment(x))\n",
    "small_df['lemma_text'] = small_df.Text.apply(lemmatize_sentence)\n",
    "\n",
    "X = np.array(small_df[\"lemma_text\"]) ## input data, object (string)\n",
    "y = np.array(small_df[\"label\"]) ## label, int64 \n",
    "nb = MultinomialNB() # Instantierer Naives Bayes\n",
    "svm = SVC(kernel='linear')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None)\n",
    "\n",
    "def data_modelling_2(models,vect):\n",
    "    result_table=[]\n",
    "    for i in range(len(models)):\n",
    "        X_train_vect = vect.fit_transform(X_train)\n",
    "        X_test_vect = vect.transform(X_test)\n",
    "        model = models[i]\n",
    "        model.fit(X_train_vect, y_train)\n",
    "        y_pred_class = model.predict(X_test_vect)\n",
    "        \n",
    "        data={'Accuracy score':metrics.accuracy_score(y_test, y_pred_class)}\n",
    "        \n",
    "        result_table.append(data)  \n",
    "        \n",
    "    df_2 = pd.DataFrame(result_table, index =['Naive Bayes', 'Support Vector Machine'])\n",
    "    return df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_modelling_2(models, vectCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM vs Naive Bayes\n",
    "\n",
    "\n",
    "We have experimented with the amount of datasets the model should use as training data. In the first model 'data_modelling' the amount of datasets is set to 2000 reviwes for each label, resulting in 10000 datasets in total. Through various tests of our models, we can observe that both models perform evenly with very little difference in accuracy. Accuracy for both of them is around 67-71 which fluctuates for each test. However, the second model that is displayed is using 200 datasets from each label, totalling in 1000 datasets. 'data_modelling_2' is then trained on 20% of the datasets. This results in a decrease of accuracy of around 15% when using both algorithms. \n",
    "\n",
    "\n",
    "Due to the results, we believe that the SVM algorithm is the better choice for this project. Our initial plan for this project was to train the two models on the original dataset containing more than half a million datapoints. However, due to hardware limitations this was not possible. Thereby we found the Naive Bayes algorithm as suitable solution for our case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our approach\n",
    "\n",
    "Our method which classifies reviews as positive, negative or neutral is based on users’ score ranged between 1-5. The method is based on our own considerations of what would be a fair way to classify the sentiment of reviews, which lead us to use a treshold-based-approach: Reviews with a score below 3 are classified as negative, reviews with a score of 3 are classified as neutral, and reviews with a score above 3 are classified as positive.\n",
    "\n",
    "We believed that a single treshold value for neutral was appropriate for our dataset, but it may not result in a nuanced labelled dataset.\n",
    "\n",
    "An approach to get a more nuanced classification would be to expand the number of sentiment classes from 3 to 5:\n",
    "\n",
    "- 1 - Very negative\n",
    "- 2 - Negative\n",
    "- 3 - Neutral\n",
    "- 4 - Postive\n",
    "- 5 - Very positive\n",
    "\n",
    "This would capture a wider range of sentiments expressed and a more nuanced classification. This might aswell improve the accuracy of our models, however it may require more resources to implement in our methods and longer compile time.\n",
    "\n",
    "Another approach we could have implemented is binary classification, where reviews are classified as positive or negative. Reviews with a score of 3 would be removed from the dataset, as they would not fit into either the positive or negative category:\n",
    "\n",
    "- 1-2 - Negative\n",
    "- 4-5 - Positive\n",
    "\n",
    "This approach would have worked well if the goal was to distinguish between positive and negative sentiment of reviews. I would simplify the classification\tand probably reduce the complexity of the method. But as a contrast to the before mentioned approach, it would not bring a very nuanced classification or capture any neutral sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "In this assignment, we analyzed and discussed the use of the Naive Bayes and Support Vector Machine algorithms for machine learning classification tasks.\n",
    "\n",
    "We found that using both algorithms with a dataset of 10,000 datapoints was almost equally efficient in terms of the accuracy of both models. However, we conclude that the support vector machine algorithm is more sufficient due to its flexibility in working with datasets of different sizes. This is because the accuracy of the second model built on the support vector machine algorithm is about 15% higher when run on a smaller set.\n",
    "\n",
    "Finally, we would like to shed light on the importance of developing models that contain different algorithms, datasets, sizes, and various approaches to classification. Depending on our approach to this assignment, we are sure we would have seen different results in terms of the model's accuracy. However, we are satisfied with our model's ability to recognize the sentiment of a review, considering our limited data science experience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
